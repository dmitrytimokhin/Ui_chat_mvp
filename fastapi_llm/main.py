import logging
import asyncio
from contextlib import asynccontextmanager
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from .models import ChatRequest, ChatResponse
from .llm_ollama import query_ollama
from .llm_qwen import query_qwen, init_models
from .utils import configure_logging, cleanup_memory, EngineError

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞ —á–µ—Ä–µ–∑ utils
configure_logging()
logger = logging.getLogger(__name__)


# Event –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
@asynccontextmanager
async def lifespan(app: FastAPI):
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∂–∏–∑–Ω–µ–Ω–Ω—ã–º —Ü–∏–∫–ª–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.
    
    –ù–∞ —Å—Ç–∞—Ä—Ç–µ: –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ –º–æ–¥–µ–ª–∏.
    –ù–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ: –ª–æ–≥–∏—Ä—É–µ—Ç –æ—Å—Ç–∞–Ω–æ–≤–∫—É.
    """
    # Startup: –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
    logger.info("üöÄ –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —Å—Ç–∞—Ä—Ç—É–µ—Ç... –û—á–∏—â–∞–µ–º –∫—ç—à –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏...")
    try:
        cleanup_memory()
    except Exception:
        logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—á–∏—Å—Ç–∏—Ç—å –∫—ç—à –ø–µ—Ä–µ–¥ —Å—Ç–∞—Ä—Ç–æ–º, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º")
    init_models()
    logger.info("‚úÖ –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é –≥–æ—Ç–æ–≤–æ –∫ —Ä–∞–±–æ—Ç–µ!")
    
    yield
    
    # Shutdown
    logger.info("üõë –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É...")


app = FastAPI(
    title="Hybrid LLM Gateway (Ollama + Qwen)",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:8501"],  # Streamlit –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    logger.info(f"–ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏: {request.model_alias}")
    try:
        if request.model_alias == "ollama":
            # model variant can be provided in request.ollama_model
            model_name = request.ollama_model or "phi"
            # run blocking I/O in threadpool
            response_text = await asyncio.to_thread(
                query_ollama,
                request.prompt,
                request.history,
                request.temperature,
                request.max_tokens,
                model_name,
            )
        elif request.model_alias == "qwen_transformers" or request.model_alias == "Qwen3":
            response_text = await asyncio.to_thread(
                query_qwen,
                request.prompt,
                request.history,
                request.temperature,
                request.max_tokens,
            )
        else:
            raise ValueError("–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–∞—è –º–æ–¥–µ–ª—å")

        return ChatResponse(response=response_text)

    except EngineError as e:
        # –ü—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–µ –æ—à–∏–±–∫–∏ –æ—Ç –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ ‚Äî –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –æ—à–∏–±–∫–∏
        error_msg = str(e)
        logger.warning(f"–î–≤–∏–∂–æ–∫ –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É: {error_msg}")
        return ChatResponse(response="", error=error_msg)

    except Exception as e:
        error_msg = str(e)
        logger.exception("–ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ /chat")
        return ChatResponse(response="", error=error_msg)
        